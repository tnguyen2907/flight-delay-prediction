{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7424968-68c7-4904-801e-e2da5e215a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, Bucketizer, SQLTransformer, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from ml_utils import evaluate\n",
    "from TargetEncoder import TargetEncoder\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import numpy as np\n",
    "    \n",
    "random_seed = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b32d55c-cb58-470b-ac9b-3ba71261e07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_processing_pipeline(train_df):\n",
    "    print(\"Number of initial features:\", len(train_df.columns) - 2)     # -1 for label col, -1 for class weight col\n",
    "\n",
    "    bool_cols = [\"DepartureExtremeWeather\", \"ArrivalExtremeWeather\"]\n",
    "    # Aggregate features: Temperature: Max deviation from 10; Precipitation: Max; WindSpeed: Max\n",
    "    OPTIMAL_TEMP = 10\n",
    "\n",
    "    sql = \"SELECT *,\\n\"\n",
    "    for col in bool_cols:\n",
    "        sql += f\"\\tCAST({col} AS int) AS {col}Int,\\n\"\n",
    "    \n",
    "    for prefix in [\"Departure\", \"Arrival\"]:\n",
    "        sql += f\"\\tGREATEST(ABS({prefix}TemperatureLag2 - {OPTIMAL_TEMP}), ABS({prefix}TemperatureLag1 - {OPTIMAL_TEMP}), ABS({prefix}Temperature - {OPTIMAL_TEMP}), ABS({prefix}TemperatureLead1 - {OPTIMAL_TEMP}), ABS({prefix}TemperatureLead2 - {OPTIMAL_TEMP})) AS {prefix}TemperatureMaxDeviation,\\n\"\n",
    "        sql += f\"\\tGREATEST({prefix}PrecipitationLag2, {prefix}PrecipitationLag1, {prefix}Precipitation, {prefix}PrecipitationLead1, {prefix}PrecipitationLead2) AS {prefix}PrecipitationMax,\\n\"\n",
    "        sql += f\"\\tGREATEST({prefix}WindSpeedLag2, {prefix}WindSpeedLag1, {prefix}WindSpeed, {prefix}WindSpeedLead1, {prefix}WindSpeedLead2) AS {prefix}WindSpeedMax,\\n\"\n",
    "\n",
    "    sql = sql[:-2] + \"\\nFROM __THIS__\"\n",
    "\n",
    "    sql_transformer = SQLTransformer(statement=sql)\n",
    "    \n",
    "    bucketizer_cols = [\"DayOfMonth\", \"ScheduledDepartureTime\", \"ScheduledArrivalTime\"]\n",
    "    bucketizer_splits = [\n",
    "        [-float(\"inf\"), 10, 20, 31, float(\"inf\")],\n",
    "        [-float(\"inf\"), 300, 600, 900, 1200, 1500, 1800, 2100, float(\"inf\")],\n",
    "        [-float(\"inf\"), 300, 600, 900, 1200, 1500, 1800, 2100, float(\"inf\")],\n",
    "    ]\n",
    "\n",
    "    bucketizer = Bucketizer(\n",
    "        splitsArray=bucketizer_splits,\n",
    "        inputCols=bucketizer_cols,\n",
    "        outputCols=[f\"{col}Bucket\" for col in bucketizer_cols],\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    string_indexers_cols = [\"Quarter\", \"Month\", \"DayOfWeek\", \"AirlineName\", \"OriginState\", \"DestinationState\", \"DepartureWeatherCondition\", \"ArrivalWeatherCondition\"]\n",
    "\n",
    "    string_indexer = StringIndexer(\n",
    "        inputCols=string_indexers_cols,\n",
    "        outputCols=[f\"{col}Index\" for col in string_indexers_cols],\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    target_encoder = TargetEncoder(\n",
    "        inputCols=[\"OriginAirport\", \"DestinationAirport\"],\n",
    "        outputCols=[\"OriginAirportTargetEncoded\", \"DestinationAirportTargetEncoded\"],\n",
    "        labelCol=\"DepartureDelayed\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    numeric_cols = [col for col in train_df.columns if any(base_col in col for base_col in [\"Temperature\", \"DewPoint\", \"Humidity\", \"Precipitation\", \"WindSpeed\", \"Pressure\"])] + \\\n",
    "        [\"DepartureTemperatureMaxDeviation\", \"DeparturePrecipitationMax\", \"DepartureWindSpeedMax\", \"ArrivalTemperatureMaxDeviation\", \"ArrivalPrecipitationMax\", \"ArrivalWindSpeedMax\"] + \\\n",
    "        [\"ScheduledElapsedTime\"]\n",
    "\n",
    "    final_cols = [\"DepartureExtremeWeatherInt\", \"ArrivalExtremeWeatherInt\"] \\\n",
    "        + [f\"{col}Bucket\" for col in bucketizer_cols] \\\n",
    "        + [f\"{col}Index\" for col in string_indexers_cols] \\\n",
    "        + numeric_cols\n",
    "        \n",
    "    print(\"Number of final features:\", len(final_cols))\n",
    "        \n",
    "    vector_assembler = VectorAssembler(\n",
    "        inputCols=final_cols,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    pipeline_stages = [\n",
    "        sql_transformer,\n",
    "        bucketizer,\n",
    "        string_indexer,\n",
    "        target_encoder,\n",
    "        vector_assembler,\n",
    "    ]\n",
    "    return pipeline_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b571d96b-b19e-4bef-84b2-6c679c4492b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_class_weight(train_df):\n",
    "    pos_count = train_df.filter(F.col(\"DepartureDelayed\") == 1).count()\n",
    "    neg_count = train_df.filter(F.col(\"DepartureDelayed\") != 1).count()\n",
    "    balancing_ratio = neg_count / float(pos_count)\n",
    "\n",
    "    train_df_weighted = train_df \\\n",
    "        .withColumn( \n",
    "            \"ClassWeight\",\n",
    "            F.when(F.col(\"DepartureDelayed\") == 1, balancing_ratio).otherwise(1.0)\n",
    "        )\n",
    "    return train_df_weighted, balancing_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbb2e05-6240-4975-acc6-c512f690b091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of initial features: 76\n",
      "Number of final features: 80\n"
     ]
    }
   ],
   "source": [
    "train_df = spark.read.parquet(\"gs://flight-delay-pred-data/ml/dataset_delayed_2014_to_2024/train.parquet\")\n",
    "val_df = spark.read.parquet(\"gs://flight-delay-pred-data/ml/dataset_delayed_2014_to_2024/val.parquet\")\n",
    "\n",
    "train_df_weighted, balancing_ratio = add_class_weight(train_df)\n",
    "processing_pipeline_stages = get_processing_pipeline(train_df_weighted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f7d9f5-c6ab-4fc1-9654-714e5538c372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_eval(params):\n",
    "    maxDepth = params[\"maxDepth\"]\n",
    "    maxIter = params[\"maxIter\"]\n",
    "    stepSize = params[\"stepSize\"]\n",
    "\n",
    "    gbt = GBTClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"DepartureDelayed\",\n",
    "        stepSize=stepSize,\n",
    "        maxDepth=maxDepth,\n",
    "        maxIter=maxIter,\n",
    "        maxBins=64,\n",
    "        seed=random_seed,\n",
    "        weightCol=\"ClassWeight\"\n",
    "    )\n",
    "    model_pipeline_stages = processing_pipeline_stages + [gbt]\n",
    "    pipeline = Pipeline(stages=model_pipeline_stages)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"gbt-{stepSize}-{maxIter}-{maxDepth}\") as run:\n",
    "        model = pipeline.fit(train_df_weighted)\n",
    "        train_predictions = model.transform(train_df)\n",
    "        val_predictions = model.transform(val_df)\n",
    "        \n",
    "        train_metrics = evaluate(train_predictions)\n",
    "        val_metrics = evaluate(val_predictions)\n",
    "\n",
    "        signature = infer_signature(train_df.drop('DepartureDelayed'), train_predictions.select(\"prediction\"))\n",
    "        mlflow.spark.log_model(\n",
    "            model, \n",
    "            f\"gbt-{stepSize}-{maxIter}-{maxDepth}\", \n",
    "            signature=signature, \n",
    "            code_paths=[\"./TargetEncoder.py\"],\n",
    "            dfs_tmpdir=\"dbfs:/tmp/mlflow\"\n",
    "        )\n",
    "        \n",
    "        mlflow.log_params({\n",
    "            \"stepSize\": stepSize,\n",
    "            \"maxIter\": maxIter,\n",
    "            \"maxDepth\": maxDepth,\n",
    "            \"balancing_ratio\": balancing_ratio\n",
    "        })\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"train_accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train_precision\": train_metrics[\"precision\"],\n",
    "            \"train_recall\": train_metrics[\"recall\"],\n",
    "            \"train_f2\": train_metrics[\"f2\"],\n",
    "            \"train_prauc\": train_metrics[\"prauc\"],\n",
    "            \"val_accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val_precision\": val_metrics[\"precision\"],\n",
    "            \"val_recall\": val_metrics[\"recall\"],\n",
    "            \"val_f2\": val_metrics[\"f2\"],\n",
    "            \"val_prauc\": val_metrics[\"prauc\"]\n",
    "        })\n",
    "\n",
    "        loss = -val_metrics[\"recall\"]\n",
    "        return {\"loss\": loss, \"status\": STATUS_OK}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389b04fc-bcf2-44b1-93a5-37a303070437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"maxDepth\": hp.choice(\"maxDepth\", [5,6 , 7, 8]),\n",
    "    \"maxIter\": hp.choice(\"maxIter\", [50, 75, 100, 125, 150]),\n",
    "    \"stepSize\": hp.uniform(\"stepSize\", 0.07, 0.15)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c937ad3e-f314-44ae-b17c-0c2622ff0918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "mlflow.set_experiment(f\"/Users/{username}/experiments/gbt-2014_to_2024-classweight100\")\n",
    "\n",
    "def train_eval_wrapper(params):\n",
    "    return train_eval(params)\n",
    "\n",
    "best_params = fmin(\n",
    "    fn=train_eval_wrapper,\n",
    "    space=search_space, \n",
    "    algo=tpe.suggest,\n",
    "    max_evals=1,\n",
    "    trials=Trials(),\n",
    "    rstate=np.random.default_rng(random_seed),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "xgboosted",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "flight-delay-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
